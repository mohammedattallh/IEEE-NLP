{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Arabic_Specific_Text_Processing","metadata":{}},{"cell_type":"markdown","source":"# Diacritization\r\n\r\n**Diacritization** is the process of adding vowel marks (diacritics) to Arabic text to clarify pronunciation and meaning. In Arabic, words are often written without vowels, and diacritics help distinguish words with similar roots but different meanings. For example, \"عَلَم\" can mean \"flag\" or \"knowledge\" depending on the diacritics.\r\n\r\n## Key Points:\r\n\r\n1. **Arabic Diacritics**: \r\n   - Short vowels (Fatha, Kasra, Damma), \r\n   - Sukun (no vowel), \r\n   - Shadda (doubling consonants).\r\n   \r\n2. **Importance**: \r\n   - **Pronunciation**: Diacritics guide the correct pronunciation of words.\r\n   - **Meaning**: They help disambiguate words with similar spelling but different meanings.\r\n\r\n3. **Diacritization Process**: \r\n   - **Automatic Diacritization**: Using NLP and machine learning models to predict and add diacritics.\r\n   - **Applications**: Used in text-to-speech systems, educational tools, and search engines.\r\n\r\n4. **Challenges**: \r\n   - Ambiguity in pronunciation based on context.\r\n   - Scarcity of annotated data for diverse Arabic dialects.\r\n   \r\nDiacritization is essential for accurate interpretation, especially in automated systems, and improves understanding and clarity in Arabic text.\r\n","metadata":{}},{"cell_type":"code","source":"import re  # Importing the regular expressions (re) module, which provides functions for working with regular expressions.\n\n# Define the function to remove diacritics from Arabic text\ndef remove_diacritics(text):\n    # Compile a regular expression pattern to match Arabic diacritics (from Unicode range U+064B to U+0652)\n    arabic_diacritics = re.compile(r'[\\u064B-\\u0652]')\n    \n    # Use re.sub() to replace all matches (diacritics) in the input text with an empty string (i.e., remove them)\n    return re.sub(arabic_diacritics, '', text)\n\n# Sample Arabic text with diacritics\ntext = \"فَصْلُ الشّتاءِ جَاءَ، وَهَبَّتِ الرِّيَاحُ البَارِدَةُ، فَحَجَزَتِ السُّحُبُ السَّمَاءَ عَنِ الشَّمْسِ، وَنَزَلَتِ الأمْطَارُ تَغْسِلُ الأَرْضَ وَتُعِيدُ الْحَيَاةَ لِلزُّرُوعِ.\"\n\n# Call the remove_diacritics function to remove diacritics from the text\nresult = remove_diacritics(text)\n\n# Print the result without diacritics\nprint(result)\n  \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T13:37:38.361911Z","iopub.execute_input":"2024-11-28T13:37:38.363005Z","iopub.status.idle":"2024-11-28T13:37:38.390011Z","shell.execute_reply.started":"2024-11-28T13:37:38.362951Z","shell.execute_reply":"2024-11-28T13:37:38.388633Z"}},"outputs":[{"name":"stdout","text":"فصل الشتاء جاء، وهبت الرياح الباردة، فحجزت السحب السماء عن الشمس، ونزلت الأمطار تغسل الأرض وتعيد الحياة للزروع.\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"# Dialect Handling in NLP \r\n\r\n**Dialect Handling** refers to addressing the variations in language forms, particularly in languages with significant dialectal diversity, such as Arabic. In Arabic, multiple dialects exist across regions, each with distinct vocabulary, grammar, and pronunciation, which complicates NLP tasks like translation and sentiment analysis.\r\n\r\n## Key Points:\r\n- **Arabic Dialects**: Variations in vocabulary and structure exist between dialects (e.g., Egyptian vs. Levantine Arabic).\r\n- **Challenges**: Differences in dialects make it difficult for models to process language effectively. Additionally, there is often a lack of dialect-specific training data.\r\n- **Techniques**: Approaches like **Dialect Identification**, **Dialect-to-MSA Translation**, and **Multilingual Models** help tackle dialect challenges.\r\n- **Applications**: Key applications include **Machine Translation**, **Sentiment Analysis**, **Text-to-Speech**, and **Speech Recognition**.\r\n- **Advancements**: Models like **multilingual BERT** and datasets like **MADAR corpus** have improved the handling of dialects.\r\n- **Challenges for NLP Models**: Dialectal differences can cause performance issues, and fine-tuning models on specific dialects irious fields.\r\n","metadata":{}},{"cell_type":"code","source":"pip install transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T13:35:19.632652Z","iopub.execute_input":"2024-11-28T13:35:19.633171Z","iopub.status.idle":"2024-11-28T13:35:30.639220Z","shell.execute_reply.started":"2024-11-28T13:35:19.633131Z","shell.execute_reply":"2024-11-28T13:35:30.637692Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.43.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\n# Model name\nmodel_name = \"riotu-lab/Baligh\"\n\n# Load the model and tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n\n# Define the translation function\ndef translate_to_msa(text):\n    # Convert the input text into tokens (tokenization) with a maximum length of 512 and truncation\n    inputs = tokenizer.encode(text, return_tensors=\"pt\", max_length=512, truncation=True)\n    \n    # Generate the translation using the model (beam search is used to explore multiple possibilities)\n    outputs = model.generate(inputs, max_length=512, num_beams=4, early_stopping=True)\n    \n    # Decode the generated tokens back into readable text, skipping special tokens like padding\n    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Example text in Egyptian Arabic (عامية)\ntext = \"\"\"النهاردة كنت فاضي شوية، فقررت أروح الكافيه مع صحابي. الجلسة كانت حلوة، والجو كان لطيف. \nاتكلمنا عن حاجات زي شغلنا وأخبار الناس. بعد شوية، قررنا نتمشى في الحديقة. الجو كان جميل.\n\nرجعنا البيت كل واحد على بيته. وأنا فكرت في اليوم واتمنيت لو كان عندي وقت أكتر علشان أخرج وأتمشى.\"\"\"\n\n# Translate the text to Modern Standard Arabic (MSA)\nmsa_translation = translate_to_msa(text)\n\n# Print the original text (in Egyptian Arabic) and the translated text (in MSA)\nprint(\"النص بالعامية:\", text)\nprint(\"النص بالفصحى:\", msa_translation)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T13:38:42.409366Z","iopub.execute_input":"2024-11-28T13:38:42.409935Z","iopub.status.idle":"2024-11-28T13:38:52.967066Z","shell.execute_reply.started":"2024-11-28T13:38:42.409887Z","shell.execute_reply":"2024-11-28T13:38:52.965853Z"}},"outputs":[{"name":"stdout","text":"النص بالعامية: النهاردة كنت فاضي شوية، فقررت أروح الكافيه مع صحابي. الجلسة كانت حلوة، والجو كان لطيف. \nاتكلمنا عن حاجات زي شغلنا وأخبار الناس. بعد شوية، قررنا نتمشى في الحديقة. الجو كان جميل.\n\nرجعنا البيت كل واحد على بيته. وأنا فكرت في اليوم واتمنيت لو كان عندي وقت أكتر علشان أخرج وأتمشى.\nالنص بالفصحى: كان وقتاً طيباً عندما قررت أن أذهب للتجوّل في مقهى. كان ذلك ممتعاً. بدأنا جولة حيث لم يكن لدينا وقت لنتحدث مع بعضنا البعض. كان لطيفاً أن أتحدث مع بعضنا البعض أثناء الجلسة. كان لطيفاً أن أتحدث مع بعضنا البعض أثناء الجلسة. لقد كنت مسروراً حقاً بأن بيننا فرصة طيبة لكي أذهب وأنام.\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"# English_Specific_Text_Processing","metadata":{}},{"cell_type":"markdown","source":"# Stemming\r\n\r\n**Stemming** is the process of shortening words to their root form, so they all match the same base word. This helps in understanding different forms of a word as one.\r\n\r\n## Key Points:\r\n- **What it does**: Stemming reduces words like \"running\" and \"runner\" to the base word \"run\".\r\n- **Why use it**: It helps machines focus on the core meaning of words without worrying about their different forms.\r\n- **Common methods**: Two common algorithms are the **Porter Stemmer** and **Snowball Stemmer**.\r\n- **Where it's used**: Stemming is useful in tasks like:\r\n  - **Searching**: Helps find documents with similar meanings even if the words are different.\r\n  - **Classifying text**: Reduces the number of word variations.\r\n  - **Understanding sentiment**: Helps analyze the main feelings behind words.\r\n\r\n## Pros:\r\n- **Improves efficiency**: It makes the text simpler by reducing word forms.\r\n- **Helps find related words**: Even different forms of a word can be matched together.\r\n\r\n## Cons:\r\n- **Can over-simplify**: It may change words too much, losing important meaning.\r\n- **Not always accurate**: Sometimes it reduces words too much or chep the meaning intact.\r\n","metadata":{}},{"cell_type":"code","source":"pip install nltk\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T13:46:57.112455Z","iopub.execute_input":"2024-11-28T13:46:57.113635Z","iopub.status.idle":"2024-11-28T13:47:07.466603Z","shell.execute_reply.started":"2024-11-28T13:46:57.113501Z","shell.execute_reply":"2024-11-28T13:47:07.464928Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.2.4)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk) (1.16.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# Importing the necessary libraries\nimport nltk\nfrom nltk.stem import PorterStemmer\n\n# Downloading the punkt tokenizer models (for word tokenization)\nnltk.download('punkt')\n\n# Initializing the PorterStemmer object, which will be used to stem words\nporter_stemmer = PorterStemmer()\n\n# Defining a sample text for stemming\ntext = \"\"\"\nI love programming a lot and I like to learn new skills.\nProgramming helps me improve my abilities in problem-solving.\nI believe that learning programming is important for my future.\n\"\"\"\n\n# Tokenizing the text into individual words\nwords = nltk.word_tokenize(text)\n\n# Stemming each word using the Porter Stemmer\n# This will reduce words like 'programming' to 'program', 'skills' to 'skill', etc.\nstemmed_words = [porter_stemmer.stem(word) for word in words]\n\n# Printing the original word and its corresponding stemmed version\nfor word, stemmed_word in zip(words, stemmed_words):\n    print(f\"{word} -> {stemmed_word}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T13:47:16.260406Z","iopub.execute_input":"2024-11-28T13:47:16.261671Z","iopub.status.idle":"2024-11-28T13:47:17.685893Z","shell.execute_reply.started":"2024-11-28T13:47:16.261617Z","shell.execute_reply":"2024-11-28T13:47:17.684652Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\nI -> I\nlove -> love\nprogramming -> program\na -> a\nlot -> lot\nand -> and\nI -> I\nlike -> like\nto -> to\nlearn -> learn\nnew -> new\nskills -> skill\n. -> .\nProgramming -> program\nhelps -> help\nme -> me\nimprove -> improv\nmy -> my\nabilities -> abil\nin -> in\nproblem-solving -> problem-solv\n. -> .\nI -> I\nbelieve -> believ\nthat -> that\nlearning -> learn\nprogramming -> program\nis -> is\nimportant -> import\nfor -> for\nmy -> my\nfuture -> futur\n. -> .\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"# Lemmatization in Natural Language Processing\n\n**Lemmatization** is a process in natural language processing (NLP) where words are reduced to their base or root form. Unlike stemming, lemmatization takes into account the context and meaning of the word, providing more accurate and meaningful results.\n\n### Key Points of Lemmatization:\n1. **Context-Aware**:\n   - Lemmatization considers the part of speech of a word (e.g., verb, noun) and the word's context to return the correct base form.\n\n2. **Produces Meaningful Words**:\n   - Lemmatization results in a proper base form (lemma) of a word, unlike stemming, which may generate non-existent or incorrect words.\n\n3. **More Accurate**:\n   - It returns the correct canonical form of the word. For example:\n     - \"better\" → \"good\"\n     - \"running\" → \"run\"\n     - \"flies\" → \"fly\"\n\n4. **Slower than Stemming**:\n   - Lemmatization is computationally more expensive because it involves dictionary lookups and morphological analysis.\n","metadata":{}},{"cell_type":"code","source":"# Importing the spaCy library\nimport spacy\n\n# Loading the pre-trained English language model\nnlp = spacy.load(\"en_core_web_sm\")  \n\n# Sample text for lemmatization\ntext = \"\"\"\nThe quick brown foxes were jumping over the lazy dogs while the cats ran along the street. The dogs are bigger than the cats,\nbut the cats are much faster. Even though the foxes were tired, they still managed to run across the park. \nThe foxes had been jumping for hours, and they felt exhausted, but they were happy to have spent time in nature. \n\"\"\"\n\n# Processing the text with spaCy to tokenize and analyze it\ndoc = nlp(text)\n\n# Loop through each token (word) in the processed text (doc)\nfor token in doc:\n    # Print the original word and its lemmatized form\n    print(f\"Original word: {token.text}, Lemma: {token.lemma_}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T13:50:28.150528Z","iopub.execute_input":"2024-11-28T13:50:28.152362Z","iopub.status.idle":"2024-11-28T13:50:30.024473Z","shell.execute_reply.started":"2024-11-28T13:50:28.152311Z","shell.execute_reply":"2024-11-28T13:50:30.023065Z"}},"outputs":[{"name":"stdout","text":"Original word: \n, Lemma: \n\nOriginal word: The, Lemma: the\nOriginal word: quick, Lemma: quick\nOriginal word: brown, Lemma: brown\nOriginal word: foxes, Lemma: fox\nOriginal word: were, Lemma: be\nOriginal word: jumping, Lemma: jump\nOriginal word: over, Lemma: over\nOriginal word: the, Lemma: the\nOriginal word: lazy, Lemma: lazy\nOriginal word: dogs, Lemma: dog\nOriginal word: while, Lemma: while\nOriginal word: the, Lemma: the\nOriginal word: cats, Lemma: cat\nOriginal word: ran, Lemma: run\nOriginal word: along, Lemma: along\nOriginal word: the, Lemma: the\nOriginal word: street, Lemma: street\nOriginal word: ., Lemma: .\nOriginal word: The, Lemma: the\nOriginal word: dogs, Lemma: dog\nOriginal word: are, Lemma: be\nOriginal word: bigger, Lemma: big\nOriginal word: than, Lemma: than\nOriginal word: the, Lemma: the\nOriginal word: cats, Lemma: cat\nOriginal word: ,, Lemma: ,\nOriginal word: \n, Lemma: \n\nOriginal word: but, Lemma: but\nOriginal word: the, Lemma: the\nOriginal word: cats, Lemma: cat\nOriginal word: are, Lemma: be\nOriginal word: much, Lemma: much\nOriginal word: faster, Lemma: fast\nOriginal word: ., Lemma: .\nOriginal word: Even, Lemma: even\nOriginal word: though, Lemma: though\nOriginal word: the, Lemma: the\nOriginal word: foxes, Lemma: fox\nOriginal word: were, Lemma: be\nOriginal word: tired, Lemma: tired\nOriginal word: ,, Lemma: ,\nOriginal word: they, Lemma: they\nOriginal word: still, Lemma: still\nOriginal word: managed, Lemma: manage\nOriginal word: to, Lemma: to\nOriginal word: run, Lemma: run\nOriginal word: across, Lemma: across\nOriginal word: the, Lemma: the\nOriginal word: park, Lemma: park\nOriginal word: ., Lemma: .\nOriginal word: \n, Lemma: \n\nOriginal word: The, Lemma: the\nOriginal word: foxes, Lemma: fox\nOriginal word: had, Lemma: have\nOriginal word: been, Lemma: be\nOriginal word: jumping, Lemma: jump\nOriginal word: for, Lemma: for\nOriginal word: hours, Lemma: hour\nOriginal word: ,, Lemma: ,\nOriginal word: and, Lemma: and\nOriginal word: they, Lemma: they\nOriginal word: felt, Lemma: feel\nOriginal word: exhausted, Lemma: exhausted\nOriginal word: ,, Lemma: ,\nOriginal word: but, Lemma: but\nOriginal word: they, Lemma: they\nOriginal word: were, Lemma: be\nOriginal word: happy, Lemma: happy\nOriginal word: to, Lemma: to\nOriginal word: have, Lemma: have\nOriginal word: spent, Lemma: spend\nOriginal word: time, Lemma: time\nOriginal word: in, Lemma: in\nOriginal word: nature, Lemma: nature\nOriginal word: ., Lemma: .\nOriginal word: \n, Lemma: \n\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"# Handling Abbreviations \n\nAbbreviations are commonly used in natural language, and correctly handling them is important for improving the performance of various NLP tasks. This includes tasks like text classification, named entity recognition (NER), and machine translation. In this document, we will explore the methods to handle abbreviations in NLP.\n\n## 1. Expanding Abbreviations\n\nExpanding abbreviations into their full forms is one way to handle them. For example:\n- \"Dr.\" → \"Doctor\"\n- \"e.g.\" → \"for example\"\n- \"i.e.\" → \"that is\"\n  \nThis can be done manually using a dictionary of common abbreviations or automatically using pre-trained models for domain-specific expansions.","metadata":{}},{"cell_type":"code","source":"# Dictionary containing common abbreviations and their full forms\nabbreviations = {\n    \"Dr.\": \"Doctor\",\n    \"Mr.\": \"Mister\",\n    \"Mrs.\": \"Misses\",\n    \"Ms.\": \"Miss\",\n    \"St.\": \"Saint\",\n    \"TV\": \"Television\",\n    \"CEO\": \"Chief Executive Officer\",\n    \"ATM\": \"Automated Teller Machine\",\n    \"USA\": \"United States of America\",\n    \"UK\": \"United Kingdom\",\n    \"EU\": \"European Union\",\n    \"ID\": \"Identification\",\n    \"DOB\": \"Date of Birth\",\n    \"FAQ\": \"Frequently Asked Questions\",\n    \"RSVP\": \"Répondez s'il vous plaît\",\n    \"P.S.\": \"Postscript\",\n    \"AKA\": \"Also Known As\",\n    \"BFF\": \"Best Friends Forever\",\n    \"FYI\": \"For Your Information\",\n    \"IMO\": \"In My Opinion\",\n    \"LOL\": \"Laugh Out Loud\",\n    \"BRB\": \"Be Right Back\",\n    \"GTG\": \"Got To Go\",\n    \"OMG\": \"Oh My God\",\n    \"TMI\": \"Too Much Information\",\n    \"YOLO\": \"You Only Live Once\",\n    \"WTF\": \"What The F***\",\n    \"TGIF\": \"Thank God It's Friday\",\n    \"DIY\": \"Do It Yourself\",\n    \"LMAO\": \"Laughing My Ass Off\",\n    \"TBD\": \"To Be Determined\",\n    \"TBC\": \"To Be Continued\",\n    \"SMH\": \"Shaking My Head\",\n    \"FOMO\": \"Fear Of Missing Out\",\n    \"BRB\": \"Be Right Back\",  # Duplicated entry\n    \"LMAO\": \"Laughing My Ass Off\",  # Duplicated entry\n    \"B2B\": \"Business to Business\",\n    \"B2C\": \"Business to Consumer\",\n    \"CCTV\": \"Closed-Circuit Television\",\n    \"MIA\": \"Missing In Action\",\n    \"WIFI\": \"Wireless Fidelity\",\n    \"NFT\": \"Non-Fungible Token\",\n    \"SEO\": \"Search Engine Optimization\",\n    \"URL\": \"Uniform Resource Locator\",\n    \"IP\": \"Internet Protocol\",\n    \"HTTP\": \"Hypertext Transfer Protocol\",\n    \"DNS\": \"Domain Name System\",\n}\n\n# Input text with several abbreviations\ntext = \"\"\"In today’s tech-driven world, Wi-Fi has become a common feature everywhere, from homes to public places. \nThe CEO of major companies uses the internet to keep track of everything, from the CRM system to daily reports. \nIf you need to withdraw money from an ATM, you can simply enter your ID, and sometimes your DOB might be required for verification.\n\nIf you’re planning to travel to the USA or the UK, don’t forget to check the FAQ section on major companies' websites \nto confirm the required documents. If you're into technology, you’ve probably heard of NFTs and how to invest in them.\n\nWhen browsing the internet, make sure you're using the correct URL to ensure you’re reaching the site you want. \nFor secure browsing, always prefer HTTPS. SEO also plays a huge role in improving search engine rankings. \nAnd for keeping in touch with friends, you can always send a quick SMS or use OMG when you're feeling surprised.\n\nBut sometimes, one may feel FOMO (Fear of Missing Out) if they’re not keeping up with the news or experiencing \nTMI (Too Much Information), so it’s always good to check the CCTV to see what’s happening around you.\"\"\"\n\n# Loop through the dictionary to replace abbreviations with their full forms in the text\nfor abbrev, full_form in abbreviations.items():\n    text = text.replace(abbrev, full_form)\n\n# Print the text after replacing abbreviations\nprint(text)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Advanced_Text_Processing","metadata":{}},{"cell_type":"markdown","source":"## Multilingual_Text_Handling","metadata":{}},{"cell_type":"code","source":"pip install googletrans","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T13:03:37.689232Z","iopub.execute_input":"2024-11-28T13:03:37.689674Z","iopub.status.idle":"2024-11-28T13:03:48.234792Z","shell.execute_reply.started":"2024-11-28T13:03:37.689638Z","shell.execute_reply":"2024-11-28T13:03:48.233367Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: googletrans in /opt/conda/lib/python3.10/site-packages (4.0.0rc1)\nRequirement already satisfied: httpx==0.13.3 in /opt/conda/lib/python3.10/site-packages (from googletrans) (0.13.3)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx==0.13.3->googletrans) (2024.8.30)\nRequirement already satisfied: hstspreload in /opt/conda/lib/python3.10/site-packages (from httpx==0.13.3->googletrans) (2024.11.1)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx==0.13.3->googletrans) (1.3.1)\nRequirement already satisfied: chardet==3.* in /opt/conda/lib/python3.10/site-packages (from httpx==0.13.3->googletrans) (3.0.4)\nRequirement already satisfied: idna==2.* in /opt/conda/lib/python3.10/site-packages (from httpx==0.13.3->googletrans) (2.10)\nRequirement already satisfied: rfc3986<2,>=1.3 in /opt/conda/lib/python3.10/site-packages (from httpx==0.13.3->googletrans) (1.5.0)\nRequirement already satisfied: httpcore==0.9.* in /opt/conda/lib/python3.10/site-packages (from httpx==0.13.3->googletrans) (0.9.1)\nRequirement already satisfied: h11<0.10,>=0.8 in /opt/conda/lib/python3.10/site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans) (0.9.0)\nRequirement already satisfied: h2==3.* in /opt/conda/lib/python3.10/site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans) (3.2.0)\nRequirement already satisfied: hyperframe<6,>=5.2.0 in /opt/conda/lib/python3.10/site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans) (5.2.0)\nRequirement already satisfied: hpack<4,>=3.0 in /opt/conda/lib/python3.10/site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans) (3.0.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import re\nfrom googletrans import Translator  # Importing the Google Translator library\n\n# The text containing both Arabic and English languages\ntext = \"\"\"الصديق الأول: مرحبًا! How are you today? أتمنى أن تكون بخير.\nالصديق الثاني: أهلاً! I'm fine, شكراً على سؤالك. How about you? كيف حالك أنت؟\nالصديق الأول: الحمد لله، بخير. كان لدي يوم طويل في العمل، ولكن الآن أنا مرتاح. Do you have any plans for the weekend?\nالصديق الثاني: في الواقع، لا، لكنني كنت أفكر في الخروج لتناول القهوة مع بعض الأصدقاء. Do you wanna join?\nالصديق الأول: فكرة رائعة! Always love hanging out with friends. أين نلتقي؟\nالصديق الثاني: دعنا نلتقي في المقهى على الزاوية، لديهم قهوة رائعة.\nالصديق الأول: يبدو مثاليًا! سأكون هناك في الخامسة.\nالصديق الثاني: رائع! أراك في الخامسة إذًا. Take care!\"\"\"\n\n# A dictionary to store the translations of English words into Arabic\ntranslation_dict = {}\n\n# Function to extract English words from the text\ndef extract_english(text):\n    # Use regular expressions to extract English words from the text\n    english_words = re.findall(r'\\b[a-zA-Z]+\\b', text)\n    return english_words\n\n# Function to replace English words with their translations in the text\ndef replace_english_with_translation(text, translation_dict):\n    # Loop through the translation dictionary and replace English words with their Arabic translations\n    for eng_word, translated_word in translation_dict.items():\n        text = re.sub(rf'\\b{eng_word}\\b', translated_word, text)\n    return text\n\n# Extract all English words from the input text\nenglish_words = extract_english(text)\n\n# Create an instance of the Translator\ntranslator = Translator()\n\n# Translate each English word to Arabic and add it to the translation dictionary\nfor word in english_words:\n    # Translate each word from English to Arabic\n    translation = translator.translate(word, src='en', dest='ar').text\n    translation_dict[word] = translation\n\n# Replace English words with their Arabic translations in the original text\ntranslated_text = replace_english_with_translation(text, translation_dict)\n\n# Print the original text and the translated text\nprint(\"Original text:\")\nprint(text)\nprint(\"\\nTranslated text:\")\nprint(translated_text)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T13:02:09.043596Z","iopub.execute_input":"2024-11-28T13:02:09.044067Z","iopub.status.idle":"2024-11-28T13:02:13.291228Z","shell.execute_reply.started":"2024-11-28T13:02:09.044029Z","shell.execute_reply":"2024-11-28T13:02:13.289964Z"}},"outputs":[{"name":"stdout","text":"النص الأصلي:\nالصديق الأول: مرحبًا! How are you today? أتمنى أن تكون بخير.\nالصديق الثاني: أهلاً! I'm fine, شكراً على سؤالك. How about you? كيف حالك أنت؟\nالصديق الأول: الحمد لله، بخير. كان لدي يوم طويل في العمل، ولكن الآن أنا مرتاح. Do you have any plans for the weekend?\nالصديق الثاني: في الواقع، لا، لكنني كنت أفكر في الخروج لتناول القهوة مع بعض الأصدقاء. Do you wanna join?\nالصديق الأول: فكرة رائعة! Always love hanging out with friends. أين نلتقي؟\nالصديق الثاني: دعنا نلتقي في المقهى على الزاوية، لديهم قهوة رائعة.\nالصديق الأول: يبدو مثاليًا! سأكون هناك في الخامسة.\nالصديق الثاني: رائع! أراك في الخامسة إذًا. Take care!\n\nالنص بعد الترجمة:\nالصديق الأول: مرحبًا! كيف نكون أنت اليوم? أتمنى أن تكون بخير.\nالصديق الثاني: أهلاً! أنا'م بخير, شكراً على سؤالك. كيف عن أنت? كيف حالك أنت؟\nالصديق الأول: الحمد لله، بخير. كان لدي يوم طويل في العمل، ولكن الآن أنا مرتاح. يفعل أنت يملك أي خطط ل ال عطلة نهاية الأسبوع?\nالصديق الثاني: في الواقع، لا، لكنني كنت أفكر في الخروج لتناول القهوة مع بعض الأصدقاء. يفعل أنت تريد ينضم?\nالصديق الأول: فكرة رائعة! دائماً حب شنق خارج مع أصدقاء. أين نلتقي؟\nالصديق الثاني: دعنا نلتقي في المقهى على الزاوية، لديهم قهوة رائعة.\nالصديق الأول: يبدو مثاليًا! سأكون هناك في الخامسة.\nالصديق الثاني: رائع! أراك في الخامسة إذًا. يأخذ رعاية!\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}